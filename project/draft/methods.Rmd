## Stage 1: Detect, Reweight Leverage Points, and Calculate Robust Covariance Matrix

### Step 1: Computing Distances

A classical way to detect leverage observations in multivariate data sets is to use Mahalanobis Distance [@mahalanobis1936generalised], which measures the distance between a point ${\bf x}$ and the sample mean:
\begin{equation}
d^{2} = ({\bf x}-\bar{\bf X})'{\bf S}^{-1}({\bf x}-\bar{\bf X})
\end{equation}
where $\bar{\bf X}$ is the arithmetic mean of the data set and $\textbf{S}$ is the usual sample covariance matrix. Due to the fact that $\bar{\bf X}$ and $\textbf{S}$ is not robust, detecting outliers with Mahalanobis distance suffers from masking, meaning the failure to detect outliers due to their very presence. Alternatively, the diagonal elements of the hat matrix $\textbf{H}$ = $\bf X(\bf X^{t}X)^{-1}X^{t}$ can be used to identify leverage points. However, the hat matrix, like the classical Mahalanobis distance, still suffers from masking, which can be explained by the relation between the $h_{ii}$ and the $d_{i}$ of the $\bf {x_{i}}$:
\begin{equation}
h_{ii} = \frac{d_{i}^{2}}{n-1} + \frac{1}{n}
\end{equation}

Previous literature has suggested to replace the arithmetic mean $\bar{\bf X}$ and sample covariance matrix $\textbf{S}$ with robust estimators. @campbell1980robust proposed to use $\textit{M}$ estimators for $\bar{\bf X}$ and $\textbf{S}$. However, the breakdown point for $\textit{M}$ estimators is at most $1/(p+1)$ [@rousseeuw2011robust]. One can also consider using estimators of multivariate location and covariance that have high break-down point, such as minimum volume ellipsoid (MVE) introduced by @rousseeuw1985multivariate and minimum covariance determinant (MCD) [@rousseeuw1999fast].

Once the robust Mahalanobis distance ($d_{ri}$) for each point is computed, an observation is declared as a leverage point if
\begin{equation}
d_{ri} > \sqrt{\chi^{2}_{p, .975}}
\end{equation}
where $\chi^{2}_{p, .975}$ is the .975 quantile of a chi-squared distribution with $p$ degrees of freedom.

An alternative method to detect leverage points we considered here is called Projection Distance. Following @wilcox2017introduction, a projection-type method for detecting outliers can be described as follows. Consider a random sample and let $n$ denote the sample size.
The method begins by finding the center of the data cloud, $\hat{\zeta}$. In this paper, the marginal medians are used for $\hat{\zeta}$. Next, for a fixed point $X_{i}$, project all n points onto the line connecting the center $\hat{\zeta}$ and $X_{i}$.
The immediate goal is to compute the distance of each of the projected 
points from $\hat{\zeta}$. Let
\begin{equation}
A_{i} = X_{i} - \hat{\zeta}
\end{equation}
\begin{equation}
B_{j} = X_{j} - \hat{\zeta}
\end{equation}
where $X_{j}$ are data points other than $X_{i}$,  $A_{i}$ and $B_{j}$ are column vectors having length $p$, and let
\begin{equation}
C_{j} = \frac{A_{i}^{'}B_{j}}{B_{j}^{'}B_{j}}B_{j}
\end{equation}
$j=1,\ldots,n.$ When projecting the points onto the line between $X_{i}$ and $\hat{\zeta}$, the projection distance of the $i$th point from $\hat{\zeta}$ is
$$D_{ij} = \|C_{j}\|$$
where
\begin{equation}
    ||C_{j}\| = \sqrt{C_{j1}^{2} + \ldots + C_{jp}^{2}}
\end{equation}
Then, we used ideal fourths as the criterion to check for outliers among the $D_{ij}$ values. The $i$th point is declared an outlier if any of its n projections satisfies
\begin{equation}
D_{ij} > M_{D} + \sqrt{\chi^{2}_{0.975,p}}(q_{2}-q_{1})
\end{equation}
where $M_{D}$ is the usual sample median based on the $D_{ij}$, and $\chi^{2}_{0.975,p}$ is the 0.975 quantile of a chi-squared distribution with $p$ degrees of freedom. Repeating this process for each $i$, $i = 1, \ldots, n$, a point is declared an outlier if any of these projections satisfies the above criterion.

In this project, we chose MVE and MCD as the robust location and scatter estimators for both Mahalanobis Distance and Projection Distance. Details on MVE and MCD estimators can be found in @wilcox2011introduction.

### Step 2: Reweighting the Covariance Matrix

Both of the previously mentioned leverage points detection techniques assign distances to each observation within its data cloud, and the issue with detecting leverage points has now become a unidimensional case. After identifying the leverage points, weights are assigned to their corresponding distances to downplay their effect on the parameter estimation. We considered Huber-Type weights:
\begin{equation}
w_{1}(d) =
\begin{cases}
 1\ ,& \text{if} \space d \leq u\\
 \frac{u}{d} ,& \text{if} \space d \ge u
 \end{cases}       
\end{equation}
\begin{equation}
w_{2}(d) = \{w_{1}(d)\}^{2}/\tau
\end{equation}
where d is the corresponding distance for each observation, u is the critical value, and $\tau$ is a tuning parameter. $w_{1}(d)$ is used in computing the reweighted mean:
\begin{equation}
\hat{\mu}_{x} = \frac{\Sigma_{i=1}^{n}w_{i1}y_{i}}{\Sigma_{i=1}^{n}w_{i1}}
\end{equation}
and $w_{2}d$ is used to compute the final reweighted covariance matrix, which will be used in Stage 2:
\begin{equation}
\hat{\Sigma}_{x} = \frac{1}{n}\Sigma_{i=1}^{n}w_{i2}(y_{i}-\hat{\mu}_{x})(y_{i}-\hat{\mu}_{x})'
\end{equation}

### Stage 2: Apply Alignment on the Robust Covariance Matrix

The second stage of RAO applies alignment on obtained robust covariance matrix from the first stage. The goal is to find a model that can be used to make accurate factor mean comparisons by searching for factor mean and variance estimates that minimize the total measurement noninvariance while downweighing the impact of influential observations. 
